\section{Risk Reduction}

Though the core algorithms of MOPS have been implemented in
LSST-appropriate style, further research and development are needed.



\subsection{Long Duration Survey Performance}

Current simulations cover fairly short time periods, and therefore
emphasize the problem of initial object discovery.  In the course of
the full survey, we expect that many detected sources will be
attributed to already-discovered objects.  Because initial object
discovery phases are relatively expensive and ephemeris calculation is
relatively fast, we expect that the resource usage of the system will
decline over time, as more objects are discovered and the size of
input catalogs is reduced.  This expectation needs to be verified and
quantified.

Attribution, precovery and Moving Object management and refinement of the
Moving Object table are not yet implemented in LSST-compliant software.
Developing this software should be a significant development task.
However, we hope that by using the algorithms from the PanSTARRS MOPS
we can avoid any significant research tasks.

To test this software, we will need to generate simulated input
catalogs which span longer time periods.  Accomplishing this will
require either significant compute-resources or improved tools for
generating input catalogs.


\subsection{Future Software Development Tasks}

\subsubsection{Filtering on Trailing for Near-Earth-Object Searching}

\label{neosTrailing}

Near-Earth Objects tend to have the highest sky-plane velocity.  This
presents a significant challenge; as we increase the maximum velocity
limit of our tracklet generation, the potential for mislinkage
increases significantly, leading to higher numbers of tracklets and
increased costs.  

Fortunately, fast-moving NEOs will generate visible trails in our
images.  By requiring all tracklets to show trails consistent with
their apparent sky-plane velocity, we expect that it will be possible
to filter most false tracklet linkages, thus rendering the problem of
NEO searching manageable.

The ability to filter on trailing is dependent almost entirely on our
ability to correctly identify trails in images.  Currently, the
ability of image processing to detect trails is not well quantified.  To
remedy this, we will need to generate simulated images which include
asteroid trails and send them to image processing; further refinement
of image processing algorithms may be neccesary.



\subsubsection{Distribution/Parallelization of Software}
\label{parallelization}

To meet the needs of full operations, DayMOPS will almost certainly
need to utilize some form of parallelism in order to reduce runtime.
For some components, such as orbit determination, this should be
easily achieved; others will require closer attention to detail.  We
have planned parallel or distributed versions for most components,
which are described below.  In some cases we have also experimented
with various initial implementations, which are noted.

%% do we need this?
%% \subsubsubsection{Tools And Methods}
%% \paragraph{Multithreading and OpenMP}
%% \paragraph{Distributed Shared Memory}
%% \paragraph{Full Distribution and MPI}


\subsubsubsection{Orbit Determination} Orbit Determination is
performed per-track; we expect that it will be slow only because the
number of tracks will be very large.  Thus, we expect that Orbit
Determination should be trivially parallel; simply divide the tracks
between various machines or CPU cores. 

\subsubsubsection{Parallel FindTracklets} The findTracklets section of
our pipeline tends to run very quickly.  For this reason, we do not
anticipate that parallelizing findTracklets will be necessary at all.
However, if we choose to parallelize it, then it should be trivial to
distribute the workload; one query is performed per input detection,
and each query is naturally independent, so it should be trivial to
parallelize the work at this level.  This could be accomplished by
changing the outer ``for'' loop to a ``parallel for'' loop, e.g. using
OpenMP.  We anticipate that the detections as well as the trees over
detections should fit in memory, making explicit distribution
unnecessary.

\subsubsubsection{Parallel CollapseTracklets} It has been our
experience that the collapseTracklets phase is generally quite fast,
and will likely not need parallelization.  One tree query will be
performed per tracklet, which provides a natural axis of parallelism.
However, there is potential for some contention between CPUs as we
normally check whether an input tracklet has been collapsed already by
a prior query, and if so, do not attempt to work with it.  Dealing
with this conditional will require synchronization between CPUs, which
could hamper performance.  It might also be ignored, perhaps allowing
the redundant work to happen in order to avoid the synchronization
costs.

\subsubsubsection{Parallel LinkTracklets} We have experimented with a
parallel, shared-memory linkTracklets implementation based on OpenMP.
The linkTracklets phase creates a tree of tracklets at each image
which generated tracklets, and attempts searching using each
temporally well-separated pair of trees as endpoint trees, with
intermediate-time trees as sources of possible support.  Each of these
searches is independent, so this provides a fairly natural axis of
parallelism.  However, this does not address the potential case where
the trees become larger than memory, which we have determined to be a
plausible problem.  To deal with this, we have used the kernel-level
distributed shared-memory provided by vSMP on the Dash and Gordon
clusters.  This way, a series of distributed machines provide the
appearance of a shared-memory system for OpenMP.  It may also be
possible to accomplish a similar effect by explicitly rewriting
linkTracklets to use a user-level distributed shared memory library
and MPI.  

\subsubsubsection{Distributed LinkTracklets} In case the implicit
memory sharing is not sufficient, it may be necessary to write an
explicitly distributed linkTracklets.  This was attempted by a CS
Master's student, Matt Cleveland, working with Prof. Dave Lowenthal.
The design was well-thought out but complex, leading to slow
implementation.  The distributed version was forked off before a
variety of changes to the serial version were made, and never merged
back.

The distributed linkTracklets held only the higher levels of trees and
the tracklets of the endpoint trees on the master node.  The master
node would attempt the linking algorithm on the higher levels of the
tree until it reached a terminal point; it would then attempt to
predict the amount of work needed to complete the linking and save the
state of the searching and estimated cost to a buffer.  Periodically,
the work items in the buffer are distributed to worker nodes, with
attention given to load distribution as well as cache issues,
attempting to minimize the amount of data which must be transferred to
worker nodes.


\subsubsubsection{Parallel Subset Removal}
